{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: {'B1': 0, 'C1': 1}\n",
      "Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Done: {'B1': False, 'C1': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "import sumo_rl\n",
    "\n",
    "# Initialize the SUMO environment\n",
    "env = sumo_rl.environment.env.SumoEnvironment(\n",
    "    net_file='third.net.xml',   # Path to your network file\n",
    "    route_file='third.rou.xml',   # Path to your route file\n",
    "    use_gui=True,                         # If you want to visualize the simulation\n",
    "    num_seconds=20000,                    # Total simulation time\n",
    "    single_agent=False,                    # Single agent or multi-agent setting\n",
    "    reward_fn='diff-waiting-time',        # Reward function (waiting time, throughput, etc.)\n",
    "    min_green=5,                          # Minimum green time\n",
    "    max_green=50,                         # Maximum green time\n",
    ")\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(1000):\n",
    "    actions = {\n",
    "    'B1': env.action_spaces('B1').sample(),  # Action for traffic light B1\n",
    "    'C1': env.action_spaces('C1').sample()   # Action for traffic light C1\n",
    "    }\n",
    "\n",
    "    obs, rewards, dones, infos = env.step(actions)\n",
    "    \n",
    "    print(f\"Step: {step}\")\n",
    "    print(f\"Actions: {actions}\")\n",
    "    #print(f\"Observations: {obs}\")\n",
    "    print(f\"Rewards: {rewards}\")\n",
    "    print(f\"Done: {dones}\")\n",
    "\n",
    "    # Check if all traffic signals are done\n",
    "    if all(dones.values()):\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network with Single traffic light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sumo_rl\n",
    "env = gym.make('sumo-rl-v0',\n",
    "                net_file='third.net.xml',  \n",
    "                route_file='third.rou.xml',\n",
    "                out_csv_name='path_to_output.csv',\n",
    "                use_gui=True,\n",
    "                num_seconds=100000)\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    next_obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumo_rl\n",
    "env = sumo_rl.parallel_env(net_file='third.net.xml',\n",
    "                  route_file='third.rou.xml',\n",
    "                  use_gui=True,\n",
    "                  num_seconds=550)\n",
    "observations = env.reset()\n",
    "epo = 1 \n",
    "while env.agents:\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}  # this is where you would insert your policy\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    # if( epo % 5 == 0):\n",
    "    #     print(f\" {rewards[\"B1\"]} , {rewards[\"C1\"]} \")\n",
    "    epo += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumo_rl\n",
    "\n",
    "env = sumo_rl.environment.env.SumoEnvironment(\n",
    "    net_file='third.net.xml',\n",
    "    route_file='third.rou.xml',\n",
    "    use_gui=False,\n",
    "    num_seconds=20000,\n",
    "    single_agent=False,\n",
    "    reward_fn='diff-waiting-time',  # or any other reward function\n",
    "    min_green=5,\n",
    "    max_green=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 2: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 3: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 4: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 5: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 6: Total Rewards: {'B1': 0.0, 'C1': -6.938893903907228e-18}\n",
      "Episode 7: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 8: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 9: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 10: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 11: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 12: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 13: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 14: Total Rewards: {'B1': 1.3877787807814457e-17, 'C1': 0.0}\n",
      "Episode 15: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 16: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 17: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 18: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 19: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 20: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 21: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 22: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 23: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 24: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 25: Total Rewards: {'B1': 0.0, 'C1': -2.7755575615628914e-17}\n",
      "Episode 26: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 27: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 28: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 29: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 30: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 31: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 32: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 33: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 34: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 35: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 36: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 37: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 38: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 39: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 40: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 41: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 42: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 43: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 44: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 45: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 46: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 47: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 48: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 49: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 50: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 51: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 52: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 53: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 54: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 55: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 56: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 57: Total Rewards: {'B1': 6.938893903907228e-18, 'C1': 0.0}\n",
      "Episode 58: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 59: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 60: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 61: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 62: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 63: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 64: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 65: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 66: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 67: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 68: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 69: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 70: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 71: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 72: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 73: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 74: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 75: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 76: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 77: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 78: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 79: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 80: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 81: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 82: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 83: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 84: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 85: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 86: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 87: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 88: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 89: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 90: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 91: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 92: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 93: Total Rewards: {'B1': 0.0, 'C1': -6.938893903907228e-18}\n",
      "Episode 94: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 95: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 96: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 97: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 98: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 99: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Episode 100: Total Rewards: {'B1': 0.0, 'C1': 0.0}\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import sumo_rl\n",
    "import pickle\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, action_space, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.action_space = action_space\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = defaultdict(lambda: np.zeros(action_space.n))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return self.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.alpha * td_error\n",
    "\n",
    "def train_q_learning(env, agents, num_episodes=1000):\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        total_rewards = {agent_id: 0 for agent_id in agents}\n",
    "        done = False\n",
    "\n",
    "        for step in range(500):\n",
    "            actions = {}\n",
    "            for agent_id, agent in agents.items():\n",
    "                state = tuple(obs[agent_id])  # Convert state to a tuple\n",
    "                action = agent.choose_action(state)\n",
    "                actions[agent_id] = action\n",
    "\n",
    "            next_obs, rewards, done, _ = env.step(actions)\n",
    "            \n",
    "            for agent_id, agent in agents.items():\n",
    "                state = tuple(obs[agent_id])\n",
    "                next_state = tuple(next_obs[agent_id])\n",
    "                agent.update_q_table(state, actions[agent_id], rewards[agent_id], next_state)\n",
    "                total_rewards[agent_id] += rewards[agent_id]\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Total Rewards: {total_rewards}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "def test_agent(env, agents):\n",
    "    obs = env.reset()\n",
    "    total_rewards = {agent_id: 0 for agent_id in agents}\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        actions = {}\n",
    "        for agent_id, agent in agents.items():\n",
    "            state = tuple(obs[agent_id])\n",
    "            action = np.argmax(agent.q_table[state])\n",
    "            actions[agent_id] = action\n",
    "\n",
    "        obs, rewards, done, _ = env.step(actions)\n",
    "        \n",
    "        for agent_id in agents:\n",
    "            total_rewards[agent_id] += rewards[agent_id]\n",
    "\n",
    "    print(f\"Total Rewards during testing: {total_rewards}\")\n",
    "\n",
    "\n",
    "    # Initialize the environment\n",
    "env = sumo_rl.environment.env.SumoEnvironment(\n",
    "        net_file='third.net.xml',\n",
    "        route_file='third.rou.xml',\n",
    "        use_gui=False,\n",
    "        num_seconds=500,\n",
    "        single_agent=False,  # Multi-agent setting\n",
    "        reward_fn='diff-waiting-time',\n",
    "        min_green=5,\n",
    "        max_green=50,\n",
    "    )\n",
    "\n",
    "    # Define the agents\n",
    "agents = {\n",
    "        'B1': QLearningAgent(action_space=env.action_spaces('B1')),\n",
    "        'C1': QLearningAgent(action_space=env.action_spaces('C1')),\n",
    "        # Add more agents if needed\n",
    "    }\n",
    "\n",
    "    # Train the agents\n",
    "train_q_learning(env, agents, num_episodes=100)\n",
    "\n",
    "QT = { 'B1' : agents['B1'].q_table ,\n",
    "        'C1' : agents['C1'].q_table }\n",
    "\n",
    "\n",
    "    # Save Q-tables\n",
    "    # with open(\"q_tables.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump({QT}, f)\n",
    "\n",
    "    # # Load Q-tables\n",
    "    # with open(\"q_tables.pkl\", \"rb\") as f:\n",
    "    #     loaded_q_tables = pickle.load(f)\n",
    "    #     for agent_id in agents:\n",
    "    #         agents[agent_id].q_table = loaded_q_tables[agent_id]\n",
    "\n",
    "    # Test the agents\n",
    "    # test_agent(env, agents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
